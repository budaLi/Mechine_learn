参照贪心学院 机器学习高阶课程大纲 进行学习
资金充足的情况下鼓励大家报名参加专业的培训~
学习时要把参考资料仔细过一下哦，不重要的部分只要了解就好啦
## 一.机器学习基础与凸优化
#### 1.1 kNN和Weighted kNN
    参考资料：
              https://blog.csdn.net/qq_43416572/article/details/100582970
              https://www.cnblogs.com/bigmonkey/p/7387943.html
              https://www.cnblogs.com/jyroy/p/9427977.html
     学习要求：
                1.简述knn算法的流程
                    对于一个未知的数据，从已知数据集中求出每个数据与其的"距离",取其中最接近的k个,然后通过多数表决的形式，即统计k个数据中种类最多的一个类，
                    我们认为这个未知数据为该类别。
                2.训练集和测试集是什么？
                    训练集可以理解为已知的带有标签的数据，测试集理解为未知数据。
                3.k值大小有什么影响？什么值最合适？
                    k值太小不具备抗干扰性 可能最近的几个k值中有噪音
                    k值太大不具备代表性 相当于较大领域中训练 近似误差较大
                    一般的k取值不会超过20，上限是n的开方，理论上训练集越大，k值越大。
                4.有哪些常用的度量距离呢？
                    欧氏距离，余弦值，相关度，曼哈顿距离(城市街区距离)。
                5.加权knn? 加的什么权？
                    反比例函数加权，高斯加权等等。个人感觉不必了解过多。
                    假如k=3
                    三个类别分别为A、A、B
                    一般来讲，A有2个，B有1个，那么判别结果为A
                    加权情况下，三个邻近的权重分别为A（0.8），A（0.6），B（0.5）
                    相当于最后有0.8+0.6=1.4（个）A，0.5（个）B，所以最后选A。

                    #通过结合KNN本身的分类算法以及对前k个距离加权，来达到分类的目的 wk-nnc算法是对经典knn算法的改进，
                    # 这种方法是对k个近邻的样本按照他们距离待分类样本的远近给一个权值w w(i) = (h(k) - h(i)) / (h(k) - h(1))
                    w(i)是第i个近邻的权值，其中1<i<k,h(i)是待测样本距离第i个近邻的距离


####  1.2 Approximated KNN算法
      个人觉得，只做简单了解就好，不必深究。
      Ann, Approximate Nearest Neighbor的缩写，就是近似最近邻搜索。
      参考资料：
                https://blog.csdn.net/suibianshen2012/article/details/101517801
                https://zhuanlan.zhihu.com/p/37381294
                https://www.ryanligod.com/2018/11/27/2018-11-27%20HNSW%20%E4%BB%8B%E7%BB%8D/#more
      学习要求：
                1.近似最近邻算法出现的意义是什么？
                       对于传统的knn算法，我们需要根据输入值去训练集中求出k个最近邻，通过多数表决的方式决定其类别。
                       在机器学习领域，语义检索，图像识别，推荐系统等方向常涉及到的一个问题是：给定一个向量X=[x1,x2,x3...xn]，
                       需要从海量的向量库中找到最相似的前K个向量。通常这些向量的维度很高，对于在线服务，用传统的方法查找是非常耗时的，
                       容易使得时延上成为瓶颈，因此业界通用的方式就是将最相似的查找转换成Ann问题。
                2.怎么样衡量其好坏?
                       衡量Ann算法好坏的一个依据是召回率，也就是通过Ann算法返回的k个结果与通过暴力查找的k的结果进行比较，如果完全一致，
                       则说明Ann算法有效。因为它节省了搜索时间效果却依然有效。
                3.有哪些ann算法？
                        目前的Ann算法有基于图的，基于树的，基于哈希等。

####  1.3 KD树,近似KD树
       简单了解就好~
       KD树是一种二叉树数据结构，可以用来进行高效的KNN计算。
       参考资料：
                https://www.joinquant.com/view/community/detail/dd60bd4e89761b916fe36dc4d14bb272
                推荐：https://zhuanlan.zhihu.com/p/23966698

#### 1.4 Locality Sensitive Hashing（局部敏感哈希 LSH）
      相关资料：
               https://colobu.com/2018/08/16/locality-sensitive-hashing/
               https://zhuanlan.zhihu.com/p/80638247
       作用：
               海量高维数据相似性度量算法
       核心思想：
                在原空间中很近（相似）的两个点，在经过LSH哈希函数哈希后的值，大概率是一样的，会放在一个桶中，相反，两个不相似的值，它们的哈希值
                相等的概率很小（很小？难道还是会出现在一个桶中？）。
                这样在高维数据集中计算两个数据的相似度，就可以在一个桶中查找，而不用和每个点进行对比。

### 1.5 线性回归模型
      相关资料：
                https://zhuanlan.zhihu.com/p/40141010
                https://easyai.tech/ai-definition/linear-regression/
       学习要求：
                1. 什么是回归分析？
                    它属于机器学习中监督学习领域中的回归。
                    目的是为了预测，比如根据一系列已知因素预测明天的天气，股票走势等。
                    它是一种预测建模技术的方法，研究因变量与自变量的关系，最后可以通过建模根据自变量预测出因变量。
                 2. 有哪几种回归？
                    <1 线性回归。线性回归通过使用最佳的拟合直线（又被称为回归线），建立因变量（Y）和一个或多个自变量（X）之间的关系。
                    <2 逻辑回归(本质是分类)。逻辑回归广泛用于分类问题。逻辑回归不要求因变量和自变量之间是线性关系，它可以处理多类型关系，
                    因为它对预测输出进行了非线性 log 变换。
                    <3 多项式回归，逐步回归等
                 3. 如何选择合适的模型？
                    当你只知道一两种技巧时，生活通常是简单的。
                    假如我们只了解线性回归和逻辑回归，我们就会想：如果结果是连续的，使用线性回归；如果结果是二值的，使用逻辑回归！
                    然而，可供选择的选项越多，选择合适的答案就越困难。类似的情况也发生在回归模型选择中。
                    在多种类型的回归模型中，基于自变量和因变量的类型、数据维数和数据的其它本质特征，选择最合适的技术是很重要的。
                    以下是如何选择合适的回归模型的几点建议：
                    <1.确定各变量的关系和影响。
                    <2.比较适合于不同模型的拟合程度，我们可以分析它们不同的指标参数。
                    <3.交叉验证是评价预测模型的最佳方法。
                    <4. ...

### 1.6 Bias-Variance Trade-off 偏差方差权衡
        个人理解为：模型过拟合和欠拟合权衡
        参考资料：
                https://liam.page/2017/03/25/bias-variance-tradeoff/
        学习要求：
                1.Bias-Variance Trade-off 是什么？背景。
                    在机器学习领域，人们总是希望自己的模型尽可能的准确的描述数据背后的真实规律。通常所说的"准确"，其实就是误差小。
                    而在该领域中，人们一般会遇到三种误差来源,随机误差、偏差和方差。随机误差是不可避免的，偏差和方差与欠拟合和过拟合紧密联系。
                    <1 随机误差：随机误差是数据本身带来的，不可避免，一般认为其服从高斯分布。
                    <2 偏差：指的是通过学习拟合出来的结果的期望值与真实规律之间的差距。偏差高欠拟合。
                    <3 方差：指的是通过学习拟合出来的结果的自身的不稳定性。方差高过拟合。
                    理想情况下，一个较好的模型的偏差和方差应较小。
                2. 欠拟合：当模型处于欠拟合状态时，根本的办法是增加模型复杂度。我们一般有以下一些办法：
                        增加模型的迭代次数；
                        更换描述能力更强的模型；
                        生成更多特征供训练使用；
                        降低正则化水平。
                   过拟合：当模型处于过拟合状态时，根本的办法是降低模型复杂度。我们则有以下一些武器：
                        扩增训练集；
                        减少训练使用的特征的数量；
                        提高正则化水平。

        参考资料已经讲的很好，建议深刻理解。

### 1.7 正则的使用：L1,L2,L-inifity Norm
        参考资料：
                https://blog.csdn.net/w5688414/article/details/78046960
                https://zhuanlan.zhihu.com/p/26884695
                https://zhuanlan.zhihu.com/p/48426076
        学习要求：
                1.l_0范数，l_1范数，l_2范数是什么 ？ 其含义是什么
                      l_0范数表示向量x中非0元素的个数。
                      l_1范数表示向量中所有元素绝对值之和。
                      l_2范数表示向量或矩阵的元素的平方和。
                1.2 可以了解下L1 LOSS和L2 lOSS
                    L1 范数损失函数，也被称为最小绝对值偏差(LAD）,最小绝对值误差（LAE）,总的来说，它是把目标值和估计值的绝对差值的总和最小化。
                    L2 范数损失函数，也被称为最小平方误差（LSE）,总的来说，它是把目标值和估计值的差值的平方和最小化。
                    对于大多数CNN网络，我们一般是使用L2-loss而不是L1-loss，因为L2-loss的收敛速度要比L1-loss要快得多。

## 二.SVM与集成模型
## 三.无监督模型与序列模型
## 四.深度学习
## 五.推荐系统与在线学习
## 六.贝叶斯模型
## 七.增强学习与其他前沿主题